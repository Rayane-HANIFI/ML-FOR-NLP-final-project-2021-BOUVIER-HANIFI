# Machine Learning for Natural Language Processing

This repository provides our work done as part of the ENSAE lecture (spring 2021): "Machine Learning for Natural Language Processing" taught by Muller Benjamin (PhD student, INRIA). 

## Objective
In this project, we aim to evaluate performances of various deep learning architectures on a hard Sequence Classification problem, which is identifying grammatical acceptability of English sentences.

In the light of our experiments, it appears that the finetuned BERT model is the only tested architecture that succeeds to converge towards an intelligent solution, its results approaching very closely the human performance on this task.

These results demonstrate once again the relevance of Transformer architectures in their ability to build efficient models for understanding the grammatical and semantic structure of language.

## Repository structure
This Github repository contains the pdf report (2 pages) and the Python Notebook code of our project.
